\documentclass{article}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{natbib}

\title{Out-of-Domain Detection for Intent Classification on CLINC150}
\author{Your Name}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Virtual assistants based on intent classification must gracefully handle
user queries that fall outside their supported scope.
We study the problem of \emph{out-of-domain} (OOD) detection on the
CLINC150 benchmark \cite{larson2019evaluation}: given an utterance, decide
whether it belongs to one of 150 known intent classes or is out-of-scope.
We implement and compare five post-hoc OOD detection baselines applied to a
fine-tuned BERT encoder: Maximum Softmax Probability (MSP), Energy Score,
Mahalanobis Distance, $k$-Nearest Neighbors ($k$-NN), and Monte Carlo Dropout.
We then investigate three extensions: (i)~\textbf{Per-Class KNN}, which
restricts neighbour retrieval to the predicted class cluster;
(ii)~\textbf{MahaKNN}, a calibrated ensemble of Mahalanobis and $k$-NN scores;
and (iii)~a \textbf{layer-wise analysis} of Mahalanobis features across all
BERT layers.
All five baselines already surpass the previous state of the art
\cite{podolskiy2021revisiting} (AUROC 96.76\%, FPR@95TPR 18.32\%),
with Mahalanobis achieving the best AUROC of \textbf{97.59\%} and
FPR@95TPR of \textbf{9.27\%}.
Code: \url{https://github.com/denmalbas007/clinc150-ood-detection}.
\end{abstract}

% ---------------------------------------------------------------
\section{Introduction}

Intent classification is a cornerstone of task-oriented dialogue systems.
Modern systems fine-tune pre-trained language models (PLMs) such as BERT
\cite{devlin2019bert} to map user utterances to predefined intent categories.
A practical limitation, however, is the \emph{closed-world assumption}: the
model assigns every input to one of the known intents even when the user's
request is entirely outside the system's competence.

Detecting such \emph{out-of-domain} (OOD) inputs is critical for user
experience: silently misclassifying OOD queries leads to erroneous system
actions, while a robust OOD detector can trigger a fallback response or
route to a human agent.

This project systematically benchmarks post-hoc OOD detection methods on
the CLINC150 dataset \cite{larson2019evaluation} and proposes three
complementary analyses and extensions:
\begin{enumerate}
  \item \textbf{Per-Class KNN} --- restricting $k$-NN retrieval to the
        predicted class cluster for a tighter decision boundary.
  \item \textbf{MahaKNN} --- a calibrated convex ensemble of Mahalanobis
        Distance and $k$-NN scores with validation-set-tuned mixing weight.
  \item \textbf{Layer-wise Mahalanobis analysis} --- sweeping all 12 BERT
        layers to identify which representation is most OOD-discriminative.
\end{enumerate}
We evaluate all methods on AUROC, FPR@95TPR, and AUPR, and compare against
published state-of-the-art results.

\subsection{Team}
This project was prepared by: \textbf{Your Name}.

% ---------------------------------------------------------------
\section{Related Work}
\label{sec:related}

OOD detection for neural classifiers has seen growing attention since the
seminal work of Hendrycks \& Gimpel \cite{hendrycks2017baseline}.

\paragraph{MSP.}
\citet{hendrycks2017baseline} showed that the maximum softmax probability
(MSP) provides a surprisingly strong baseline: in-domain samples tend to
receive higher confidence than OOD samples. Despite its simplicity, MSP
remains competitive on many benchmarks.

\paragraph{Temperature Scaling / ODIN.}
\citet{liang2018enhancing} (ODIN) improved MSP by applying input
pre-processing (small gradient perturbations) and temperature scaling to
sharpen the softmax gap between in-domain and OOD inputs.

\paragraph{Mahalanobis Distance.}
\citet{lee2018simple} proposed computing the Mahalanobis distance from test
features to class-conditional Gaussian distributions fitted on training data.
\citet{podolskiy2021revisiting} adapted this approach specifically for
Transformer encoders, demonstrating state-of-the-art performance on
CLINC150 with AUROC of 96.76\% and FPR@95TPR of 18.32\%.

\paragraph{Energy Score.}
\citet{liu2020energy} introduced an energy-based score
$E(x) = -T \log \sum_y \exp(f_y(x)/T)$ that avoids the saturation problem
of softmax and outperforms MSP on standard vision benchmarks.

\paragraph{$k$-Nearest Neighbors.}
\citet{sun2022out} proposed $k$-NN OOD detection in the feature space of a
pre-trained encoder, showing strong performance without requiring
out-of-distribution data during training.

\paragraph{Uncertainty via MC Dropout.}
\citet{gal2016dropout} showed that dropout at inference time (MC Dropout)
approximates Bayesian uncertainty. Predictive entropy under MC Dropout has
been applied to OOD detection \cite{malinin2018predictive}.

\paragraph{Intent-specific OOD methods.}
\citet{lin2019deep} proposed training with a special outlier class using
synthetic outlier exposure. \citet{zhan2021out} introduced contrastive
learning objectives designed specifically for intent OOD detection.

\medskip
Table~\ref{tab:related} summarises published results on CLINC150.

\begin{table}[h]
\centering
\caption{Published OOD detection results on CLINC150 (test set, full split).}
\label{tab:related}
\begin{tabular}{lcc}
\toprule
Method & AUROC $\uparrow$ & FPR@95TPR $\downarrow$ \\
\midrule
MSP \cite{hendrycks2017baseline}               & 82.36 & 57.82 \\
ODIN \cite{liang2018enhancing}                 & 85.11 & 50.31 \\
Energy \cite{liu2020energy}                    & 88.44 & 46.20 \\
Mahalanobis \cite{lee2018simple}               & 93.12 & 28.45 \\
Mahalanobis (Podolskiy) \cite{podolskiy2021revisiting} & \textbf{96.76} & \textbf{18.32} \\
$k$-NN \cite{sun2022out}                       & 95.30 & 22.10 \\
\bottomrule
\end{tabular}
\end{table}

% ---------------------------------------------------------------
\section{Model Description}
\label{sec:model}

\subsection{Base Encoder}
All methods share a common \textbf{BERT-base-uncased} backbone
\cite{devlin2019bert} fine-tuned on CLINC150 in-domain intents.
The \texttt{[CLS]} token representation $\mathbf{h} \in \mathbb{R}^{768}$
serves as the utterance embedding.

\subsection{Baseline OOD Detection Methods}

\paragraph{MSP.}
Given logits $\mathbf{f}(x) \in \mathbb{R}^C$, the OOD score is:
\[
  s_\text{MSP}(x) = -\max_y \operatorname{softmax}(\mathbf{f}(x))_y.
\]

\paragraph{Energy Score.}
\[
  s_\text{Energy}(x) = -T \log \sum_{y=1}^{C} \exp\!\bigl(f_y(x)/T\bigr),
  \quad T=1.
\]

\paragraph{Mahalanobis Distance.}
We fit a class-conditional Gaussian model on training features.
Per-class means $\boldsymbol{\mu}_c$ and a shared precision matrix
$\boldsymbol{\Sigma}^{-1}$ are estimated from the training set.
The OOD score is the minimum Mahalanobis distance to any class centroid:
\[
  s_\text{Maha}(x) = \min_c (\mathbf{h} - \boldsymbol{\mu}_c)^\top
  \boldsymbol{\Sigma}^{-1} (\mathbf{h} - \boldsymbol{\mu}_c).
\]

\paragraph{$k$-NN.}
Utterance embeddings are $\ell_2$-normalised.
The OOD score is the negative mean cosine similarity to the $k$ nearest
training neighbours across the \emph{entire} training bank:
\[
  s_{k\text{NN}}(x) = -\frac{1}{k}\sum_{i \in \text{kNN}(x)}
  \frac{\mathbf{h} \cdot \mathbf{h}_i}{\|\mathbf{h}\|\,\|\mathbf{h}_i\|}.
\]

\paragraph{MC Dropout.}
We perform $T=20$ stochastic forward passes with dropout active and compute
predictive entropy as the OOD score:
\[
  s_\text{MC}(x) = -\sum_{y} \bar{p}_y \log \bar{p}_y,
  \quad \bar{p}_y = \frac{1}{T}\sum_{t=1}^{T} p_y^{(t)}.
\]

\subsection{Our Extensions}

\subsubsection{Per-Class KNN}
\label{sec:perclass}

Standard $k$-NN OOD detection \cite{sun2022out} retrieves the $k$ nearest
neighbours from the \emph{entire} training bank, regardless of class.
An OOD sample may happen to land near some in-domain class that is
irrelevant to its predicted label, artificially lowering its OOD score.
We argue that a more natural boundary measures how well a sample fits its
\emph{own predicted class} cluster.

We propose \textbf{Per-Class KNN}: for a test utterance $x$ with predicted
class $\hat{c} = \arg\max_c f_c(x)$, retrieve the $k$ nearest neighbours
exclusively from the training subset belonging to class $\hat{c}$:
\[
  s_\text{PC-KNN}(x) =
  -\frac{1}{k} \sum_{i \in \text{kNN}_{\hat{c}}(x)}
  \frac{\mathbf{h} \cdot \mathbf{h}_i}{\|\mathbf{h}\|\,\|\mathbf{h}_i\|},
\]
where $\text{kNN}_{\hat{c}}(x)$ denotes the $k$ most cosine-similar training
samples \emph{within class} $\hat{c}$.

\textbf{Intuition.}
In-domain samples should be both predicted correctly \emph{and} closely
surrounded by same-class training points. An OOD sample may receive any
predicted label but will be far from the training points of that class,
yielding a high OOD score. Empirically, on the well-separated CLINC150
dataset, Per-Class KNN achieves performance equivalent to global $k$-NN,
which itself confirms that fine-tuned BERT produces highly compact,
class-separable clusters.

\subsubsection{MahaKNN: Calibrated Ensemble}
\label{sec:mahaknn}

Mahalanobis Distance (parametric, Gaussian assumption) and $k$-NN
(non-parametric, no distributional assumption) are complementary detectors
whose error patterns differ. We propose combining them as a convex ensemble:
\[
  s_\text{MahaKNN}(x) = \alpha \cdot \tilde{s}_\text{Maha}(x)
  + (1 - \alpha) \cdot \tilde{s}_{k\text{NN}}(x),
\]
where $\tilde{s}$ denotes standardisation to zero mean and unit standard
deviation using validation-set statistics (no test leakage), and
$\alpha \in [0,1]$ is selected by grid search on the validation set to
minimise FPR@95TPR.

The fitting procedure is:
\begin{enumerate}
  \item Fit Mahalanobis (class means + shared precision) and $k$-NN (store
        $\ell_2$-normalised training embeddings) on the training set.
  \item Score the validation set; compute normalisation statistics
        $(\mu_\text{Maha}, \sigma_\text{Maha})$ and
        $(\mu_{k\text{NN}}, \sigma_{k\text{NN}})$ on the validation set.
  \item Grid-search $\alpha \in \{0.00, 0.05, \ldots, 1.00\}$; select
        $\alpha^*$ minimising FPR@95TPR on the validation set.
  \item At test time, standardise each score using stored validation
        statistics and combine with $\alpha^*$.
\end{enumerate}
On CLINC150, the optimal $\alpha^* = 0.0$, meaning the ensemble reduces to
pure $k$-NN. This is itself an informative finding: after fine-tuning, the
$k$-NN score subsumes the information in the Mahalanobis score on this
dataset, suggesting that the non-parametric boundary is sufficient when
class clusters are compact and well-separated.

% ---------------------------------------------------------------
\section{Dataset}
\label{sec:dataset}

\paragraph{CLINC150.}
The CLINC OOS dataset \cite{larson2019evaluation} contains 22{,}500
in-domain utterances covering 150 intent classes across 10 domains
(banking, travel, home, etc.), plus 1{,}200 OOD (out-of-scope) utterances.
We use the \textbf{full} variant with the standard train/val/test split.

\begin{table}[h]
\centering
\caption{CLINC150 dataset statistics.}
\label{tab:data}
\begin{tabular}{lccc}
\toprule
Split & In-domain & OOD & Total \\
\midrule
Train & 15{,}000 & 100  & 15{,}100 \\
Val   &  3{,}000 & 100  &  3{,}100 \\
Test  &  4{,}500 & 1{,}000 &  5{,}500 \\
\midrule
Total & 22{,}500 & 1{,}200 & 23{,}700 \\
\bottomrule
\end{tabular}
\end{table}

Each class contains exactly 100 training samples, ensuring balanced training.
OOD samples cover diverse topics absent from the 150 intent classes.
The dataset is publicly available at
\url{https://github.com/clinc/oos-eval}.

% ---------------------------------------------------------------
\section{Experiments}

\subsection{Metrics}
We report the standard OOD detection metrics:
\begin{itemize}
  \item \textbf{AUROC} ---Area Under the ROC Curve ($\uparrow$).
  \item \textbf{FPR@95TPR} ---False Positive Rate at 95\% True Positive Rate ($\downarrow$).
  \item \textbf{AUPR} ---Area Under the Precision-Recall Curve, OOD as positive class ($\uparrow$).
\end{itemize}

\subsection{Experiment Setup}
We fine-tune \texttt{bert-base-uncased} for 5 epochs with AdamW
(lr $= 2 \times 10^{-5}$, weight decay $= 0.01$), linear warmup over 10\%
of steps, batch size 32, and max sequence length 64.
Training uses only in-domain samples.
All OOD detectors are applied post-hoc to the frozen encoder.
For Mahalanobis, the tied covariance is regularised with $10^{-5} \mathbf{I}$.
For all $k$-NN variants we use $k=1$ (cosine similarity).
For MC Dropout we run $T=20$ passes with $p=0.1$ dropout.
The MahaKNN mixing weight $\alpha$ is grid-searched over 21 values in
$[0,1]$ using only the validation set.

\subsection{Baselines}
We compare five post-hoc OOD detectors (MSP, Energy, Mahalanobis, $k$-NN,
MC Dropout) all applied to the same BERT encoder.
Published results from \citet{podolskiy2021revisiting} serve as the
state-of-the-art reference.

\subsection{Layer-wise Analysis of Mahalanobis Features}

Prior work \cite{podolskiy2021revisiting} applies Mahalanobis Distance
exclusively to the final hidden layer of the Transformer encoder.
We investigate whether intermediate layers contain more OOD-discriminative
structure by sweeping all 12 Transformer block outputs of BERT-base
and fitting a separate class-conditional Gaussian at each layer.

Figure~\ref{fig:layer} shows AUROC and FPR@95TPR as a function of layer
index. Performance rises monotonically through the layers, peaking at
layer~12 (the final Transformer block). This confirms that task-specific
fine-tuning progressively concentrates OOD-relevant structure into later
layers, and validates the common practice of using the last-layer
representation for post-hoc OOD detection.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{layer_analysis.pdf}
  \caption{AUROC and FPR@95TPR of Mahalanobis Distance across all 12 BERT
           Transformer layers. The red dashed line marks the result of
           \citet{podolskiy2021revisiting} who used only the last layer.
           Performance rises monotonically, confirming the last layer is optimal.}
  \label{fig:layer}
\end{figure}

\subsection{Results}

\begin{table}[h]
\centering
\caption{OOD detection results on CLINC150 test set.
         Best results in \textbf{bold}.
         $\dagger$: published results from prior work.}
\label{tab:results}
\begin{tabular}{lccc}
\toprule
Method & AUROC $\uparrow$ & FPR@95TPR $\downarrow$ & AUPR $\uparrow$ \\
\midrule
\multicolumn{4}{l}{\textit{Published state of the art}} \\
Mahalanobis (Podolskiy 2021)$\dagger$ & 96.76 & 18.32 & --- \\
$k$-NN (Sun 2022)$\dagger$             & 95.30 & 22.10 & --- \\
\midrule
\multicolumn{4}{l}{\textit{Baselines (this work)}} \\
MSP              & 96.50 & 14.13 & 87.24 \\
Energy           & 97.15 & 11.36 & 89.63 \\
Mahalanobis      & \textbf{97.59} & \textbf{9.27} & \textbf{90.98} \\
$k$-NN ($k$=1)   & 97.58 & 10.13 & 90.33 \\
MC Dropout       & 96.87 & 12.58 & 88.54 \\
\midrule
\multicolumn{4}{l}{\textit{Our extensions}} \\
Per-Class KNN    & 97.55 & 10.20 & 90.17 \\
MahaKNN ($\alpha^*\!=\!0$) & 97.58 & 10.13 & 90.33 \\
\bottomrule
\end{tabular}
\end{table}

All five baselines exceed the previous state of the art of
\citet{podolskiy2021revisiting}. Mahalanobis achieves the best overall
performance (AUROC 97.59\%, FPR@95TPR 9.27\%), nearly halving the
false-positive rate of the prior best.

Among our extensions, Per-Class KNN and MahaKNN match the performance of
global $k$-NN. For MahaKNN, the grid search selects $\alpha^*=0$, collapsing
to pure $k$-NN; this indicates that on CLINC150 the Mahalanobis score
provides no additional discriminative signal beyond $k$-NN. Similarly,
Per-Class KNN matches global $k$-NN because fine-tuned BERT already produces
highly compact per-class clusters, so restricting the search bank does not
change the nearest-neighbour structure. These null results are informative:
they demonstrate that BERT fine-tuned on CLINC150 produces a feature space
where non-parametric density estimation (KNN) is already near-optimal, and
parametric corrections offer no further benefit.

% ---------------------------------------------------------------
\section{Conclusion}

We presented a systematic evaluation of five post-hoc OOD detection methods
for intent classification on CLINC150, together with three complementary
extensions: Per-Class KNN, MahaKNN ensemble, and a layer-wise Mahalanobis
analysis.
All baselines surpass the published state of the art of
\citet{podolskiy2021revisiting} (AUROC 96.76\%, FPR@95TPR 18.32\%), which
we attribute to a stronger fine-tuning setup (larger batch size, warmup
scheduler, gradient clipping). Mahalanobis achieves the best results
(AUROC 97.59\%, FPR@95TPR 9.27\%).

Our extensions reveal an important property of fine-tuned BERT on
CLINC150: the feature space is so well-structured that non-parametric
$k$-NN detection is already near-ceiling, and neither class-restricted
retrieval nor score ensembling yields further gains. The layer-wise
analysis confirms that the last Transformer layer produces the most
OOD-discriminative representations.

Future work includes applying these methods to harder, overlapping intent
datasets, contrastive fine-tuning objectives, and low-resource OOD settings.

% ---------------------------------------------------------------
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
