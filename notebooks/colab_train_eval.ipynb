{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Detection for Intent Classification \u2014 CLINC150\n",
    "### Full pipeline: data \u2192 fine-tune BERT \u2192 evaluate all OOD methods\n",
    "\n",
    "**Runtime:** GPU (Runtime \u2192 Change runtime type \u2192 T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate\n",
    "\n",
    "import subprocess, sys\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout if result.returncode == 0 else 'No GPU detected \u2014 switch runtime to GPU!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/denmalbas007/clinc150-ood-detection.git\n",
    "%cd clinc150-ood-detection\n",
    "!python scripts/download_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dataset import load_clinc150, CLINC150Dataset\n",
    "from models import IntentClassifier, MCDropoutClassifier\n",
    "from metrics import compute_all_metrics\n",
    "from methods.msp import compute_msp_scores\n",
    "from methods.energy import compute_energy_scores\n",
    "from methods.mahalanobis import fit_mahalanobis, compute_mahalanobis_scores\n",
    "from methods.knn import fit_knn, compute_knn_scores\n",
    "from methods.mc_dropout import compute_mc_dropout_scores\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Config\n",
    "MODEL_NAME   = 'bert-base-uncased'\n",
    "EPOCHS       = 5\n",
    "BATCH_SIZE   = 64\n",
    "LR           = 2e-5\n",
    "MAX_LEN      = 64\n",
    "SEED         = 42\n",
    "CKPT_PATH    = Path('checkpoints/best_model.pt')\n",
    "CKPT_PATH.parent.mkdir(exist_ok=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, label2id = load_clinc150()\n",
    "num_classes = len(label2id)\n",
    "print(f'Intent classes: {num_classes}')\n",
    "\n",
    "for split, samples in splits.items():\n",
    "    n_in  = sum(1 for *_, is_ood in samples if not is_ood)\n",
    "    n_ood = sum(1 for *_, is_ood in samples if is_ood)\n",
    "    print(f'  {split:5s}: {n_in:5d} in-domain | {n_ood:4d} OOD')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def make_loader(split_name, shuffle=False):\n",
    "    ds = CLINC150Dataset(splits[split_name], label2id, tokenizer, MAX_LEN)\n",
    "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
    "                      num_workers=2, pin_memory=True)\n",
    "\n",
    "train_loader = make_loader('train', shuffle=True)\n",
    "val_loader   = make_loader('val')\n",
    "test_loader  = make_loader('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MCDropoutClassifier(MODEL_NAME, num_classes).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    ctx = torch.enable_grad() if train else torch.no_grad()\n",
    "    with ctx:\n",
    "        for batch in tqdm(loader, leave=False):\n",
    "            ids   = batch['input_ids'].to(DEVICE)\n",
    "            mask  = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            # skip OOD rows\n",
    "            keep = labels != -1\n",
    "            if keep.sum() == 0: continue\n",
    "            ids, mask, labels = ids[keep], mask[keep], labels[keep]\n",
    "\n",
    "            logits = model(ids, mask)\n",
    "            loss   = criterion(logits, labels)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            correct    += (logits.argmax(-1) == labels).sum().item()\n",
    "            total      += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['val_loss'].append(vl_loss)\n",
    "    history['train_acc'].append(tr_acc)\n",
    "    history['val_acc'].append(vl_acc)\n",
    "\n",
    "    print(f'Epoch {epoch}/{EPOCHS} | '\n",
    "          f'Train loss={tr_loss:.4f} acc={tr_acc:.4f} | '\n",
    "          f'Val   loss={vl_loss:.4f} acc={vl_acc:.4f}')\n",
    "\n",
    "    if vl_acc > best_val_acc:\n",
    "        best_val_acc = vl_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'label2id': label2id,\n",
    "            'model_name': MODEL_NAME,\n",
    "            'num_classes': num_classes,\n",
    "            'val_acc': vl_acc,\n",
    "        }, CKPT_PATH)\n",
    "        print(f'  \u2713 Saved checkpoint (val_acc={vl_acc:.4f})')\n",
    "\n",
    "print(f'\\nBest val accuracy: {best_val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "epochs_x = range(1, EPOCHS + 1)\n",
    "\n",
    "ax1.plot(epochs_x, history['train_loss'], label='Train')\n",
    "ax1.plot(epochs_x, history['val_loss'],   label='Val')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs_x, history['train_acc'], label='Train')\n",
    "ax2.plot(epochs_x, history['val_acc'],   label='Val')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy'); ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/training_curves.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "print(f\"Loaded checkpoint (val_acc={ckpt['val_acc']:.4f})\")\n",
    "\n",
    "# Ground truth for test set\n",
    "is_ood_gt = np.array([int(s[2]) for s in splits['test']])\n",
    "print(f'Test: {(is_ood_gt==0).sum()} in-domain, {is_ood_gt.sum()} OOD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OOD Detection \u2014 All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need train_loader without shuffle for fitting Mahalanobis/KNN\n",
    "train_loader_eval = make_loader('train', shuffle=False)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# --- MSP ---\n",
    "print('MSP...')\n",
    "scores = compute_msp_scores(model, test_loader, DEVICE).numpy()\n",
    "results['MSP'] = compute_all_metrics(scores, is_ood_gt)\n",
    "\n",
    "# --- Energy ---\n",
    "print('Energy...')\n",
    "scores = compute_energy_scores(model, test_loader, DEVICE).numpy()\n",
    "results['Energy'] = compute_all_metrics(scores, is_ood_gt)\n",
    "\n",
    "# --- Mahalanobis ---\n",
    "print('Mahalanobis (fitting)...')\n",
    "class_means, precision = fit_mahalanobis(model, train_loader_eval, num_classes, DEVICE)\n",
    "scores = compute_mahalanobis_scores(model, test_loader, class_means, precision, DEVICE).numpy()\n",
    "results['Mahalanobis'] = compute_all_metrics(scores, is_ood_gt)\n",
    "\n",
    "# --- KNN k=1 ---\n",
    "print('KNN (k=1, fitting)...')\n",
    "train_feats = fit_knn(model, train_loader_eval, DEVICE)\n",
    "scores = compute_knn_scores(model, test_loader, train_feats, DEVICE, k=1).numpy()\n",
    "results['KNN (k=1)'] = compute_all_metrics(scores, is_ood_gt)\n",
    "\n",
    "# --- KNN k=10 ---\n",
    "print('KNN (k=10)...')\n",
    "scores = compute_knn_scores(model, test_loader, train_feats, DEVICE, k=10).numpy()\n",
    "results['KNN (k=10)'] = compute_all_metrics(scores, is_ood_gt)\n",
    "\n",
    "# --- MC Dropout ---\n",
    "print('MC Dropout (20 passes)...')\n",
    "scores = compute_mc_dropout_scores(model, test_loader, DEVICE, n_passes=20).numpy()\n",
    "results['MC Dropout'] = compute_all_metrics(scores, is_ood_gt)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6b. Per-Class KNN \u2014 Our Method\n",
    "\n",
    "Extension of Sun et al. (2022): retrieve neighbours **only from the predicted class** instead of the full training bank.\n",
    "An OOD sample is flagged if it is far from the cluster of its own predicted class, eliminating false negatives caused by proximity to irrelevant classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from methods.per_class_knn import fit_per_class_knn, compute_per_class_knn_scores\n",
    "\n",
    "print('Fitting Per-Class KNN...')\n",
    "class_banks = fit_per_class_knn(model, train_loader_eval, num_classes, DEVICE)\n",
    "print(f'  Banks fitted for {len(class_banks)} classes')\n",
    "\n",
    "print('Scoring test set...')\n",
    "pc_knn_scores = compute_per_class_knn_scores(\n",
    "    model, test_loader, class_banks, DEVICE, k=1\n",
    ").numpy()\n",
    "\n",
    "pc_knn_metrics = compute_all_metrics(pc_knn_scores, test_is_ood)\n",
    "results['Per-Class KNN (ours)'] = pc_knn_metrics\n",
    "print(f\"  AUROC={pc_knn_metrics['AUROC']:.4f}  \"\n",
    "      f\"FPR@95={pc_knn_metrics['FPR@95TPR']:.4f}  \"\n",
    "      f\"AUPR={pc_knn_metrics['AUPR']:.4f}\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results \u2014 All Methods\n",
    "\n",
    "Comparison table including baselines and our Per-Class KNN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Published SotA for reference\n",
    "published = {\n",
    "    'MSP (Hendrycks 2017)':        {'AUROC': 0.8236, 'FPR@95TPR': 0.5782, 'AUPR': None},\n",
    "    'Energy (Liu 2020)':           {'AUROC': 0.8844, 'FPR@95TPR': 0.4620, 'AUPR': None},\n",
    "    'Mahalanobis (Podolskiy 2021)':{'AUROC': 0.9676, 'FPR@95TPR': 0.1832, 'AUPR': None},\n",
    "    'KNN (Sun 2022)':              {'AUROC': 0.9530, 'FPR@95TPR': 0.2210, 'AUPR': None},\n",
    "}\n",
    "\n",
    "print(f\"{'Method':<35} {'AUROC':>8} {'FPR@95':>8} {'AUPR':>8}\")\n",
    "print('\u2500' * 62)\n",
    "print('Published:')\n",
    "for method, m in published.items():\n",
    "    aupr_s = '  N/A  ' if m['AUPR'] is None else f\"{m['AUPR']:.4f}\"\n",
    "    print(f\"  {method:<33} {m['AUROC']:>8.4f} {m['FPR@95TPR']:>8.4f} {aupr_s:>8}\")\n",
    "print('Ours:')\n",
    "for method, m in results.items():\n",
    "    print(f\"  {method:<33} {m['AUROC']:>8.4f} {m['FPR@95TPR']:>8.4f} {m['AUPR']:>8.4f}\")\n",
    "\n",
    "# Save\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('\\nSaved to results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Re-collect raw scores for ROC curves\n",
    "raw_scores = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    raw_scores['MSP']         = compute_msp_scores(model, test_loader, DEVICE).numpy()\n",
    "    raw_scores['Energy']      = compute_energy_scores(model, test_loader, DEVICE).numpy()\n",
    "    raw_scores['Mahalanobis'] = compute_mahalanobis_scores(model, test_loader, class_means, precision, DEVICE).numpy()\n",
    "    raw_scores['KNN (k=1)']   = compute_knn_scores(model, test_loader, train_feats, DEVICE, k=1).numpy()\n",
    "    raw_scores['MC Dropout']  = compute_mc_dropout_scores(model, test_loader, DEVICE, n_passes=20).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC curves\n",
    "colors = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00']\n",
    "for (method, scores), color in zip(raw_scores.items(), colors):\n",
    "    fpr, tpr, _ = roc_curve(is_ood_gt, scores)\n",
    "    auroc_val = results[method]['AUROC']\n",
    "    axes[0].plot(fpr, tpr, label=f'{method} ({auroc_val:.3f})', color=color, lw=1.8)\n",
    "axes[0].plot([0,1],[0,1],'k--',lw=1)\n",
    "axes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\n",
    "axes[0].set_title('ROC Curves \u2014 OOD Detection on CLINC150')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# AUROC bar chart with SotA reference\n",
    "methods_all  = list(results.keys())\n",
    "aurocs_ours  = [results[m]['AUROC'] * 100 for m in methods_all]\n",
    "sota_auroc   = 96.76  # Podolskiy 2021\n",
    "\n",
    "bars = axes[1].bar(methods_all, aurocs_ours,\n",
    "                   color=['#4daf4a' if v >= sota_auroc else '#377eb8' for v in aurocs_ours])\n",
    "axes[1].axhline(sota_auroc, color='red', linestyle='--', lw=1.5,\n",
    "                label=f'SotA (Podolskiy 2021): {sota_auroc:.2f}%')\n",
    "for bar, val in zip(bars, aurocs_ours):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                 f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "axes[1].set_ylabel('AUROC (%)')\n",
    "axes[1].set_title('AUROC Comparison (green = beats SotA)')\n",
    "axes[1].set_xticklabels(methods_all, rotation=25, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(70, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/ood_results.pdf', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Update Report with Real Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-fill result numbers into the LaTeX report\n",
    "tex_path = Path('report/report.tex')\n",
    "tex = tex_path.read_text()\n",
    "\n",
    "replacements = {\n",
    "    'MSP (ours)':         'MSP',\n",
    "    'Energy (ours)':      'Energy',\n",
    "    'Mahalanobis (ours)': 'Mahalanobis',\n",
    "    '$k$-NN k=1 (ours)':  'KNN (k=1)',\n",
    "    'MC Dropout (ours)':  'MC Dropout',\n",
    "}\n",
    "\n",
    "for tex_name, key in replacements.items():\n",
    "    if key not in results:\n",
    "        continue\n",
    "    m = results[key]\n",
    "    auroc_s = f\"{m['AUROC']*100:.2f}\"\n",
    "    fpr_s   = f\"{m['FPR@95TPR']*100:.2f}\"\n",
    "    aupr_s  = f\"{m['AUPR']*100:.2f}\"\n",
    "    # Replace placeholder XX.XX in the row that starts with tex_name\n",
    "    old = f'{tex_name} & XX.XX & XX.XX & XX.XX'\n",
    "    new = f'{tex_name} & {auroc_s} & {fpr_s} & {aupr_s}'\n",
    "    tex = tex.replace(old, new)\n",
    "\n",
    "tex_path.write_text(tex)\n",
    "print('report/report.tex updated with real numbers!')\n",
    "\n",
    "# Print final table\n",
    "df = pd.DataFrame(results).T * 100\n",
    "df.index.name = 'Method'\n",
    "print(df.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save to Google Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to mount Drive and save checkpoint\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import shutil\n",
    "# shutil.copy('checkpoints/best_model.pt', '/content/drive/MyDrive/clinc150_best_model.pt')\n",
    "# shutil.copy('results.json', '/content/drive/MyDrive/clinc150_results.json')\n",
    "# print('Saved to Google Drive')\n",
    "print('Uncomment the block above to save to Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Commit results back to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your git identity first\n",
    "!git config user.email \"you@example.com\"\n",
    "!git config user.name \"Your Name\"\n",
    "\n",
    "!git add results.json report/report.tex report/training_curves.pdf report/ood_results.pdf\n",
    "!git commit -m \"Add training results and figures\"\n",
    "\n",
    "# To push: uncomment and set your token\n",
    "# import os\n",
    "# token = 'ghp_YOUR_TOKEN'\n",
    "# !git remote set-url origin https://{token}@github.com/denmalbas007/clinc150-ood-detection.git\n",
    "# !git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Layer-wise Mahalanobis Analysis\n\nWe analyse which BERT layer produces the best OOD-discriminative features.\nPodolskiy (2021) only used the last layer \u2014 we sweep all 13 hidden states.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from methods.mahalanobis import layer_wise_mahalanobis\n\nprint('Running layer-wise Mahalanobis (this takes ~3-5 min)...')\nlayer_scores, layer_metrics = layer_wise_mahalanobis(\n    model, train_loader_eval, test_loader,\n    is_ood_gt, num_classes, DEVICE, num_layers=12\n)\n\n# Table\nprint(f\"\\n{'Layer':>8} {'AUROC':>8} {'FPR@95':>8} {'AUPR':>8}\")\nprint('\u2500' * 38)\nfor layer_idx in sorted(layer_metrics.keys()):\n    m = layer_metrics[layer_idx]\n    label = f'{layer_idx} (last)' if layer_idx == 13 else str(layer_idx)\n    print(f\"  {label:>8} {m['AUROC']:>8.4f} {m['FPR@95TPR']:>8.4f} {m['AUPR']:>8.4f}\")\n\nbest_layer = max(layer_metrics, key=lambda l: layer_metrics[l]['AUROC'])\nprint(f\"\\nBest layer: {best_layer}  AUROC={layer_metrics[best_layer]['AUROC']:.4f}\")\n\n# Save layer metrics\nwith open('layer_metrics.json', 'w') as f:\n    json.dump({str(k): v for k, v in layer_metrics.items()}, f, indent=2)\nprint('Saved to layer_metrics.json')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Layer-wise visualization\nlayers = sorted(layer_metrics.keys())\naurocs  = [layer_metrics[l]['AUROC'] * 100  for l in layers]\nfprs    = [layer_metrics[l]['FPR@95TPR'] * 100 for l in layers]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n\nax1.plot(layers, aurocs, 'o-', color='#377eb8', lw=2, ms=6)\nax1.axhline(96.76, color='red', ls='--', lw=1.5, label='Podolskiy 2021 (last layer)')\nax1.set_xlabel('BERT Layer'); ax1.set_ylabel('AUROC (%)')\nax1.set_title('AUROC by Layer \u2014 Mahalanobis Distance')\nax1.set_xticks(layers)\nax1.legend(); ax1.grid(alpha=0.3)\n\nax2.plot(layers, fprs, 's-', color='#e41a1c', lw=2, ms=6)\nax2.axhline(18.32, color='red', ls='--', lw=1.5, label='Podolskiy 2021 (last layer)')\nax2.set_xlabel('BERT Layer'); ax2.set_ylabel('FPR@95TPR (%)')\nax2.set_title('FPR@95TPR by Layer \u2014 Mahalanobis Distance')\nax2.set_xticks(layers)\nax2.legend(); ax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('report/layer_analysis.pdf', bbox_inches='tight', dpi=150)\nplt.show()\nprint(f'Best layer by AUROC: {best_layer}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}